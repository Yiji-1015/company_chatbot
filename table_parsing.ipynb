{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4390de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import fitz\n",
    "from bs4 import BeautifulSoup\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93dea736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(input_file, batch_size):\n",
    "    # Open input_pdf\n",
    "    input_pdf = fitz.open(input_file)\n",
    "    num_pages = len(input_pdf)\n",
    "    print(f\"Total number of pages: {num_pages}\")\n",
    " \n",
    "    # Split input_pdf\n",
    "    for start_page in range(0, num_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, num_pages) - 1\n",
    " \n",
    "        # Write output_pdf to file\n",
    "        input_file_basename = os.path.splitext(input_file)[0]\n",
    "        output_file = f\"{input_file_basename}_{start_page}_{end_page}.pdf\"\n",
    "        print(output_file)\n",
    "        with fitz.open() as output_pdf:\n",
    "            output_pdf.insert_pdf(input_pdf, from_page=start_page, to_page=end_page)\n",
    "            output_pdf.save(output_file)\n",
    " \n",
    "    # Close input_pdf\n",
    "    input_pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a0799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages: 10\n",
      "lloydk_Q&A_0_0.pdf\n",
      "lloydk_Q&A_1_1.pdf\n",
      "lloydk_Q&A_2_2.pdf\n",
      "lloydk_Q&A_3_3.pdf\n",
      "lloydk_Q&A_4_4.pdf\n",
      "lloydk_Q&A_5_5.pdf\n",
      "lloydk_Q&A_6_6.pdf\n",
      "lloydk_Q&A_7_7.pdf\n",
      "lloydk_Q&A_8_8.pdf\n",
      "lloydk_Q&A_9_9.pdf\n"
     ]
    }
   ],
   "source": [
    "# Input arguments\n",
    "input_file = \"lloydk_Q&A.pdf\" # Replace with a file of your own\n",
    "batch_size = 1  # Maximum available value is 100\n",
    "split_pdf(input_file, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a47189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테이블을 구분하는 코드\n",
    "\n",
    "tables = []\n",
    "paragraphs = []\n",
    "\n",
    "for i in range(10):\n",
    "    filename = f\"lloydk_Q&A_{i}_{i}.pdf\"\n",
    "    url = \"https://api.upstage.ai/v1/document-digitization\"\n",
    "    headers = {\"Authorization\": f\"Bearer {upstage_api_key}\"}\n",
    "    files = {\"document\": open(filename, \"rb\")}\n",
    "    data = {\"ocr\": \"force\", \"base64_encoding\": \"['table']\", \"model\": \"document-parse\"}\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "    for element in response.json()['elements']:\n",
    "        if element['category'] == 'table':\n",
    "            table_html = element['content'][\"html\"]\n",
    "            soup = BeautifulSoup(table_html, \"html.parser\")\n",
    "            # rows = []\n",
    "            for tr in soup.find_all(\"tr\"):\n",
    "                cells = [\n",
    "                    td.get_text(strip=True).replace(\"\\n\", \" \")\n",
    "                    for td in tr.find_all([\"td\", \"th\"])\n",
    "                ]\n",
    "                paragraphs.append(cells)\n",
    "            # for r in rows:\n",
    "            #     paragraphs.append(r)\n",
    "        else:\n",
    "            html_str = element['content']['html']\n",
    "            if html_str.startswith('\"') and html_str.endswith('\"'):\n",
    "                html_str = html_str[1:-1]\n",
    "            soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=False)\n",
    "            paragraphs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f64028",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = []\n",
    "current_type = None   # 'text' 혹은 'table'\n",
    "current_items = []\n",
    "\n",
    "def item_type(item):\n",
    "    # 문자열이면 문장\n",
    "    if isinstance(item, str):\n",
    "        return \"text\"\n",
    "    # 리스트면 (여기서는) 테이블\n",
    "    if isinstance(item, list):\n",
    "        return \"table\"\n",
    "    return \"other\"\n",
    "\n",
    "for item in paragraphs:\n",
    "    t = item_type(item)\n",
    "\n",
    "    # text / table / other 타입이 바뀌면 이전 블록을 마무리\n",
    "    if t != current_type:\n",
    "        if current_items:\n",
    "            blocks.append({\n",
    "                \"type\": current_type,\n",
    "                \"items\": current_items\n",
    "            })\n",
    "        current_type = t\n",
    "        current_items = []\n",
    "\n",
    "    current_items.append(item)\n",
    "\n",
    "# 마지막 블록도 추가\n",
    "if current_items:\n",
    "    blocks.append({\n",
    "        \"type\": current_type,\n",
    "        \"items\": current_items\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f42891",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_embedding = []\n",
    "for i in range(len(blocks)):\n",
    "    texts = []\n",
    "    if blocks[i]['type'] == 'text':\n",
    "        texts.append(blocks[i]['items'])\n",
    "        text_for_embedding.append(texts[0])\n",
    "    else:\n",
    "        texts = []\n",
    "        texts.append(blocks[i-1]['items'][-1:]+blocks[i]['items'])\n",
    "        text_for_embedding.append(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a855a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_char_with_overlap(lines, max_chars=700, overlap_chars=50):\n",
    "    \"\"\"\n",
    "    lines: 이미 문장 단위로 나뉜 리스트 (list[str])\n",
    "    max_chars: 청크 하나의 최대 글자 수\n",
    "    overlap_chars: 다음 청크로 넘길 최소 글자 수 (겹치는 분량)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current = []\n",
    "    cur_len = 0\n",
    "\n",
    "    for raw in lines:\n",
    "        line = raw.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        line_len = len(line)\n",
    "\n",
    "        # 이 줄을 더하면 max_chars를 넘는 경우 → 지금까지 걸 하나의 청크로 확정\n",
    "        if current and (cur_len + line_len > max_chars):\n",
    "            # 현재 청크 저장\n",
    "            chunks.append(\"\\n\".join(current))\n",
    "\n",
    "            # 🔁 오버랩 부분 만들기: 뒤에서부터 overlap_chars 이상이 될 때까지 가져오기\n",
    "            overlap = []\n",
    "            overlap_len = 0\n",
    "            for s in reversed(current):\n",
    "                if overlap_len + len(s) > overlap_chars and overlap:\n",
    "                    break\n",
    "                overlap.append(s)\n",
    "                overlap_len += len(s)\n",
    "            overlap = list(reversed(overlap))\n",
    "\n",
    "            current = overlap[:]          # 새 청크는 오버랩으로 시작\n",
    "            cur_len = sum(len(s) for s in current)\n",
    "\n",
    "        # 현재 청크에 이 줄 추가\n",
    "        current.append(line)\n",
    "        cur_len += line_len\n",
    "\n",
    "    # 마지막 청크 처리\n",
    "    if current:\n",
    "        chunks.append(\"\\n\".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b94e484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_chunks = []\n",
    "for chunks in text_for_embedding:\n",
    "    if type(chunks[1]) == list:\n",
    "        list_for_chunks.append(chunks)\n",
    "        continue\n",
    "    else:\n",
    "        num_chunks = chunk_by_char_with_overlap(chunks)\n",
    "        for num in num_chunks:\n",
    "            list_for_chunks.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4496e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY가 .env에 없어요 ㅠㅠ\")\n",
    "\n",
    "client_oa = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Qdrant 클라이언트 (로컬)\n",
    "client_qd = QdrantClient(\n",
    "    url=\"http://localhost:6333\",  # 또는 host=\"localhost\", port=6333\n",
    ")\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "EMBED_DIM = 1536  # text-embedding-3-small의 차원 수\n",
    "COLLECTION_NAME = \"lloydk_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b963c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking collection existence...\n",
      "→ Exists? True\n",
      "⚙️ Collection already exists: lloydk_docs\n"
     ]
    }
   ],
   "source": [
    "def ensure_collection():\n",
    "    print(\"🔍 Checking collection existence...\")\n",
    "    exists = client_qd.collection_exists(collection_name=COLLECTION_NAME)\n",
    "    print(f\"→ Exists? {exists}\")\n",
    "\n",
    "    if not exists:\n",
    "        client_qd.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=EMBED_DIM, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(f\"✅ Created collection: {COLLECTION_NAME}\")\n",
    "    else:\n",
    "        print(f\"⚙️ Collection already exists: {COLLECTION_NAME}\")\n",
    "        \n",
    "ensure_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a4f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_batch(texts):\n",
    "    \"\"\"\n",
    "    texts: list[str]\n",
    "    return: list[list[float]] (임베딩 벡터들)\n",
    "    \"\"\"\n",
    "    # 빈 문자열 제거 + 전처리\n",
    "    cleaned = []\n",
    "    idx_map = []\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        if not isinstance(t, str):\n",
    "            t = str(t)\n",
    "        t = t.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        cleaned.append(t)\n",
    "        idx_map.append(i)\n",
    "\n",
    "    if not cleaned:\n",
    "        return [], []\n",
    "\n",
    "    resp = client_oa.embeddings.create(\n",
    "        model=EMBED_MODEL,\n",
    "        input=cleaned,\n",
    "    )\n",
    "\n",
    "    vectors = [d.embedding for d in resp.data]\n",
    "    return vectors, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14aa25a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_for_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mlist_for_chunks\u001b[49m), BATCH_SIZE):\n\u001b[0;32m      3\u001b[0m     batch \u001b[38;5;241m=\u001b[39m list_for_chunks[i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[0;32m      4\u001b[0m     vectors, cleaned \u001b[38;5;241m=\u001b[39m embed_batch(batch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_for_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "for i in range(0, len(list_for_chunks), BATCH_SIZE):\n",
    "    batch = list_for_chunks[i:i+BATCH_SIZE]\n",
    "    vectors, cleaned = embed_batch(batch)\n",
    "\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=vec,\n",
    "            payload={\"text\": text},\n",
    "        )\n",
    "        for text, vec in zip(cleaned, vectors)\n",
    "    ]\n",
    "\n",
    "    client_qd.upsert(collection_name=\"lloydk_docs\", points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbad25cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.584\n",
      "text: · 컨텍스트 윈도우 (Context Window)\n",
      "컨텍스트 윈도우는 한 번에 처리할 수 있는 입력 길이로, 대화나 문맥을 얼마나 길게 기억할 수 있는 지를 뜻합니다.\n",
      "· LLM의 사양과 성능 간의 상관관계\n",
      "일반적으로 파라미터 수가 많을수록 성능이 좋아지지만, 일정 수준 이상에서는 데이터 품질, 학습 방 법이 더 중요해집니다. 즉, 크기가 성능에 영향을 주긴 하지만, '무조건 클수록 좋은 건 아님'이 최근 트렌드입니다.\n",
      "· 사양과 비용 간의 상관관계\n",
      "파라미터 수가 많을수록 연산량과 메모리 사용량이 급증해 학습·추론 비용도 높아집니다. 따라서 고사양 모델은 높은 성능을 주지만, 실사용 시 운영비용과 응답 속도 문제도 함께 고려해야 합니다.\n",
      "· 대표적인 LLM 모델들의 파라미터 수와 컨텍스트 윈도우 길이\n",
      "\n",
      "score: 0.548\n",
      "text: ['· 대표적인 LLM 모델들의 파라미터 수와 컨텍스트 윈도우 길이', ['모델명', '파라미터 수', '컨텍스트 윈 도우', '특징', '적합한 용도'], ['Llama 4 (Maverick)', '400B', '1M', '초대형 다국 어 대응, 멀티 모달 처리', '글로벌 대용 량 문서 분석, RAG'], ['DeepSeek R1', '671B (MoE)', '128K', '추론 특화, 비 용 효율', '고난도 질문 응답, 분석 도 구'], ['Claude 3.7 Sonnet', '(비공개)', '350K', '안전성과 판 단력 강화', '챗봇, 문서 정 리, 비서형 AI'], ['Mistral Small 3.1', '24B', '128K', '가볍고 빠름, 오픈소스', '내부 시스템 연동, 엣지 디 바이스'], ['Phi-4', '14.7B', '16K', '저사양 환경 대응', '임베디드 AI, 비용 민감 환 경'], ['GPT-4o', '1.8T (추정)', '2M', '텍스트+이미 지 동시 처리', '최고 성능, 복 합 AI 서비스'], ['XGen-7B', '7B', '8K', '중소규모 조 직용', '문서 요약, 간 단 질의응답']]\n",
      "\n",
      "score: 0.467\n",
      "text: DO 솔루션 관련 기술 개념에 대한 내부 질의응답\n",
      "1. VectorDB와 Vector Embedding Model이란 무엇인가?\n",
      "벡터 임베딩 모델(embedding model)은 텍스트나 이미지 같은 비수학적 데이터를 머신러닝 모델에 서 처리할 수 있도록 숫자 배열(벡터)로 변환해 주는 모델을 말한다.\n",
      "예를 들어 문장 임베딩 모델은 문장 간 유사도를 반영한 고차원 벡터를 생성할 수 있다.\n",
      "한편 벡터 데이터베이스(VectorDB)는 임베딩을 통해 생성된 고차원 벡터를 효율적으로 저장하고 유 사도 기반 검색을 제공하는 특수 데이터베이스다.\n",
      "벡터 DB는 최근접이웃 알고리즘(k-NN 인덱스 예. HNSW, IVF 등)를 사용해 쿼리(검색어) 벡터와 유 사한(가까운) 데이터 포인트를 빠르게 찾고, 일반 DB처럼 데이터 관리·인증·접근 제어 기능도 제공한 다.\n",
      "2. LLM 모델 관련 사양은 어떻게 되는가?\n",
      "LLM(대규모 언어 모델)은 파라미터 수와 컨텍스트 윈도 크기(최대 토큰 수)가 주요 사양이다.\n",
      "· 파라미터 수 = LLM 모델 사양(크기)\n",
      "LLM의 크기는 내부에 존재하는 파라미터 수로 결정되며, 파라미터가 많을수록 더 복잡한 문맥과 개 념을 학습할 수 있습니다. 즉, 파라미터 수는 모델이 얼마나 정교하고 깊이 있게 사고할 수 있는지를 나타냅니다.\n",
      "· 컨텍스트 윈도우 (Context Window)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"컨텍스트 윈도우가 가장 긴 언어모델\"\n",
    "q_emb, _ = embed_batch([query])\n",
    "\n",
    "results = client_qd.query_points(\n",
    "    collection_name=\"lloydk_docs\",\n",
    "    query=q_emb[0],   # query_vector → query\n",
    "    limit=3,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "for r in results.points:\n",
    "    print(f\"score: {r.score:.3f}\")\n",
    "    print(f\"text: {r.payload['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "907f7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat():\n",
    "    print(\"RAG 챗봇입니다. 'exit' 입력 시 종료.\\n\")\n",
    "    while True:\n",
    "        q = input(\"👤 질문: \").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        if q.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            print(\"bye~\")\n",
    "            break\n",
    "\n",
    "        # 1️⃣ 쿼리 임베딩\n",
    "        q_emb, _ = embed_batch([q])\n",
    "\n",
    "        # 2️⃣ Qdrant에서 검색\n",
    "        results = client_qd.query_points(\n",
    "            collection_name=\"lloydk_docs\",\n",
    "            query=q_emb[0],\n",
    "            limit=5,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        # 3️⃣ 검색된 문서 모으기\n",
    "        contexts = [r.payload[\"text\"] for r in results.points if \"text\" in r.payload]\n",
    "        if not contexts:\n",
    "            print(\"🤖 관련 문서를 찾지 못했어요 ㅠㅠ\\n\")\n",
    "            continue\n",
    "\n",
    "        # 4️⃣ 답변 생성\n",
    "        context_text = \"\\n\\n\".join(contexts)\n",
    "        system_prompt = (\n",
    "            \"너는 DO 솔루션 관련 기술 문서를 바탕으로 답변하는 어시스턴트야.\\n\"\n",
    "            \"반드시 아래 제공된 문맥 내에서만 답변해. 모르면 모른다고 말해.\\n\"\n",
    "            \"table의 경우, [[열1, 열2], [열1에 대한 아이템, 열2에 대한 아이템]...] 이런 식으로 되어있으니 반드시 끝까지 다 보고 답변해야 해.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"[질문]\\n{q}\\n\\n[관련 문서]\\n{context_text}\\n\\n\"\n",
    "            \"위 내용을 기반으로 한국어로 자연스럽게 답변해줘.\"\n",
    "        )\n",
    "\n",
    "        resp = client_oa.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        print(\"\\n🤖 답변:\\n\", resp.choices[0].message.content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bef5787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 챗봇입니다. 'exit' 입력 시 종료.\n",
      "\n",
      "\n",
      "🤖 답변:\n",
      " 현재 컨텍스트 윈도우가 가장 긴 언어 모델은 \"GPT-4o\"로, 2M의 컨텍스트 윈도우를 가지고 있습니다. 이 모델은 텍스트와 이미지를 동시에 처리할 수 있는 복합 AI 서비스에 적합한 최고 성능의 모델로 알려져 있습니다. \n",
      "\n",
      "bye~\n"
     ]
    }
   ],
   "source": [
    "rag_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88943a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
