# backend/app.py
import os
# from dotenv import load_dotenv
from qdrant_client import QdrantClient
from openai import OpenAI
import streamlit as st

# load_dotenv()
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
client_oa = OpenAI(api_key=OPENAI_API_KEY)
# Qdrant í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬)
client_qd = QdrantClient(
    url="http://localhost:6333",  # ë˜ëŠ” host="localhost", port=6333
)

EMBED_MODEL = "text-embedding-3-small"
EMBED_DIM = 1536
COLLECTION_NAME = "lloydk_docs"


def embed_batch(texts):
    """
    texts: list[str]
    return: list[list[float]] (ì„ë² ë”© ë²¡í„°ë“¤)
    """
    cleaned = []
    idx_map = []

    for i, t in enumerate(texts):
        if not isinstance(t, str):
            t = str(t)
        t = t.strip()
        if not t:
            continue
        cleaned.append(t)
        idx_map.append(i)

    if not cleaned:
        return [], []

    resp = client_oa.embeddings.create(
        model=EMBED_MODEL,
        input=cleaned,
    )

    vectors = [d.embedding for d in resp.data]
    return vectors, cleaned


def rag_answer(question: str) -> str:
    """
    í•œ ë²ˆ ì§ˆë¬¸ -> í•œ ë²ˆ ë‹µë³€ ë°˜í™˜ (ìŠ¤íŠ¸ë¦¼ë¦¿/ì›¹ì—ì„œ ì‚¬ìš©í•˜ê¸° ì¢‹ê²Œ)
    """
    question = question.strip()
    if not question:
        return "ì§ˆë¬¸ì´ ë¹„ì–´ ìˆì–´ìš”. ë­˜ ë¬¼ì–´ë³¼ì§€ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™‚"

    # 1ï¸âƒ£ ì¿¼ë¦¬ ì„ë² ë”©
    q_emb, _ = embed_batch([question])
    if not q_emb:
        return "ì„ë² ë”©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ë‹¤ì‹œ í•œë²ˆ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    # 2ï¸âƒ£ Qdrantì—ì„œ ê²€ìƒ‰
    results = client_qd.query_points(
        collection_name=COLLECTION_NAME,
        query=q_emb[0],
        limit=5,
        with_payload=True
    )

    # 3ï¸âƒ£ ê²€ìƒ‰ëœ ë¬¸ì„œ ëª¨ìœ¼ê¸°
    contexts = [r.payload["text"] for r in results.points if "text" in r.payload]
    if not contexts:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ í•´ë³¼ê¹Œìš”? ğŸ¤”"

    # 4ï¸âƒ£ ë‹µë³€ ìƒì„±
    context_text = "\n\n".join(contexts)
    system_prompt = (
        "ë„ˆëŠ” DO ì†”ë£¨ì…˜ ê´€ë ¨ ê¸°ìˆ  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\n"
        "ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ë¬¸ë§¥ ë‚´ì—ì„œë§Œ ë‹µë³€í•´. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´.\n"
        "tableì˜ ê²½ìš°, [[ì—´1, ì—´2], [ì—´1ì— ëŒ€í•œ ì•„ì´í…œ, ì—´2ì— ëŒ€í•œ ì•„ì´í…œ]...] ì´ëŸ° ì‹ìœ¼ë¡œ ë˜ì–´ìˆìœ¼ë‹ˆ ë°˜ë“œì‹œ ëê¹Œì§€ ë‹¤ ë³´ê³  ë‹µë³€í•´ì•¼ í•´."
    )
    user_prompt = (
        f"[ì§ˆë¬¸]\n{question}\n\n[ê´€ë ¨ ë¬¸ì„œ]\n{context_text}\n\n"
        "ìœ„ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì¤˜."
    )

    resp = client_oa.chat.completions.create(
        model="gpt-5",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,
    )

    answer = resp.choices[0].message.content
    return answer


def rag_chat():
    """
    í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ìš© CLI ë²„ì „ (ì›ë˜ ìˆë˜ ê±° ìœ ì§€í•˜ê³  ì‹¶ìœ¼ë©´)
    """
    print("RAG ì±—ë´‡ì…ë‹ˆë‹¤. 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ.\n")
    while True:
        q = input("ğŸ‘¤ ì§ˆë¬¸: ").strip()
        if not q:
            continue
        if q.lower() in ["exit", "quit", "q"]:
            print("bye~")
            break

        answer = rag_answer(q)
        print("\nğŸ¤– ë‹µë³€:\n", answer, "\n")


if __name__ == "__main__":
    # í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ë©´ CLI ëª¨ë“œë¡œ
    rag_chat()